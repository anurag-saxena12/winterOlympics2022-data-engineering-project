{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86098c5-afc8-4ff5-bbfe-7388912c0f48",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Mounting Data Lake using Azure Key Vault Secrets\n",
    "\n",
    "clientID = dbutils.secrets.get(\"secretScope\", \"clientIDSecret\")\n",
    "clientSecret = dbutils.secrets.get(\"secretScope\", \"clientSecretSecret\")\n",
    "tenantSecret = dbutils.secrets.get(\"secretScope\", \"tenantIDSecret\")\n",
    "\n",
    "#Configure Authentication\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": clientID,\n",
    "    \"fs.azure.account.oauth2.client.secret\": clientSecret,\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenantSecret}/oauth2/token\"  \n",
    "}\n",
    "\n",
    "#Mount Data Lake\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://olympic-beijing-data@olympicbeijingdata.dfs.core.windows.net\", \n",
    "    mount_point = \"/mnt/beijingolympic\",\n",
    "    extra_configs = configs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a82636-972b-475b-b977-40aad7669227",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Read each file within the staging-data folder and load them into a dataframe with headers and infer schema\n",
    "\n",
    "athletes_raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"/mnt/beijingolympic/staging-data/athletes.csv\")\n",
    "coaches_raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"/mnt/beijingolympic/staging-data/coaches.csv\")\n",
    "teams_raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"/mnt/beijingolympic/staging-data/teams.csv\")\n",
    "entries_by_discipline_raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"/mnt/beijingolympic/staging-data/entries_by_discipline.csv\")\n",
    "medals_raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"/mnt/beijingolympic/staging-data/medals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91fb29a8-abdf-4bd5-ba5f-ac24a65a7944",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#function to add an ID column to each DataFrame\n",
    "def add_id_column(df, id_column_name):\n",
    "    return df.rdd.zipWithIndex().map(lambda x: (x[1] + 1, *x[0])).toDF([id_column_name] + df.columns)\n",
    "\n",
    "# Add ID columns to each DataFrame with dynamic names\n",
    "athletes = add_id_column(athletes_raw, \"athlete_id\")\n",
    "coaches = add_id_column(coaches_raw, \"coach_id\")\n",
    "teams = add_id_column(teams_raw, \"team_id\")\n",
    "entries_by_discipline = add_id_column(entries_by_discipline_raw, \"entry_id\")\n",
    "medals = add_id_column(medals_raw, \"medal_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebdd36c-75e4-450a-bce4-50d7b89b6f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write athletes to Data Lake\n",
    "\n",
    "athlete_output_path = \"dbfs:/mnt/beijingolympic/transformed-data/athletes.csv\"\n",
    "athletes.write.option(\"header\", \"true\").mode(\"overwrite\").csv(athlete_output_path)\n",
    "metadata_path = athlete_output_path\n",
    "files = dbutils.fs.ls(metadata_path)\n",
    "\n",
    "#Deletes metadata files (although important for larger datasets, not needed for this use case)\n",
    "\n",
    "for file in files:\n",
    "    if file.name != \"athletes.csv\" and not file.name.endswith(\".csv\"):\n",
    "        dbutils.fs.rm(file.path, recurse=True)\n",
    "\n",
    "#Write coaches to Data Lake\n",
    "\n",
    "coaches_output_path = \"dbfs:/mnt/beijingolympic/transformed-data/coaches.csv\"\n",
    "coaches.write.option(\"header\", \"true\").mode(\"overwrite\").csv(coaches_output_path)\n",
    "metadata_path = coaches_output_path\n",
    "files = dbutils.fs.ls(metadata_path)\n",
    "\n",
    "for file in files:\n",
    "    if file.name != \"coaches.csv\" and not file.name.endswith(\".csv\"):\n",
    "        dbutils.fs.rm(file.path, recurse=True)\n",
    "\n",
    "#Write teams to Data Lake\n",
    "\n",
    "teams_output_path = \"dbfs:/mnt/beijingolympic/transformed-data/teams.csv\"\n",
    "teams.write.option(\"header\", \"true\").mode(\"overwrite\").csv(teams_output_path)\n",
    "metadata_path = teams_output_path\n",
    "files = dbutils.fs.ls(metadata_path)\n",
    "\n",
    "for file in files:\n",
    "    if file.name != \"teams.csv\" and not file.name.endswith(\".csv\"):\n",
    "        dbutils.fs.rm(file.path, recurse=True)\n",
    "\n",
    "#Write entries_by_discipline to Data Lake\n",
    "\n",
    "entries_by_discipline_output_path = \"dbfs:/mnt/beijingolympic/transformed-data/entries_by_discipline.csv\"\n",
    "entries_by_discipline.write.option(\"header\", \"true\").mode(\"overwrite\").csv(entries_by_discipline_output_path)\n",
    "metadata_path = entries_by_discipline_output_path\n",
    "files = dbutils.fs.ls(metadata_path)\n",
    "\n",
    "for file in files:\n",
    "    if file.name != \"entries_by_discipline.csv\" and not file.name.endswith(\".csv\"):\n",
    "        dbutils.fs.rm(file.path, recurse=True) \n",
    "\n",
    "#Write medals to Data Lake\n",
    "\n",
    "medals_output_path = \"dbfs:/mnt/beijingolympic/transformed-data/medals.csv\"\n",
    "medals.write.option(\"header\", \"true\").mode(\"overwrite\").csv(medals_output_path)\n",
    "metadata_path = medals_output_path\n",
    "files = dbutils.fs.ls(metadata_path)\n",
    "\n",
    "for file in files:\n",
    "    if file.name != \"medals.csv\" and not file.name.endswith(\".csv\"):\n",
    "        dbutils.fs.rm(file.path, recurse=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3074147144751808,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "olympic-beijing-transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
